{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Graph Convolutional Networks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def load_data(path=\"/Users/zliu/Downloads/pygcn-master/data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/tkipf/pygcn/blob/master/pygcn/utils.py\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path=\"/Users/zliu/Downloads/pygcn-master/data/cora/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolutional Layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GCN Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Training settings\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                    help='Disables CUDA training.')\n",
    "# parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "#                    help='Validate during training pass.')\n",
    "#parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "#parser.add_argument('--epochs', type=int, default=200,\n",
    "#                    help='Number of epochs to train.')\n",
    "\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=16, #Number of hidden units\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=0.5 #Dropout rate (1 - keep probability)\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.01, #Initial learning rate\n",
    "                       weight_decay=5e-4 #Weight decay (L2 loss on parameters)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.3286 acc_train: 0.9571 loss_val: 0.6477 acc_val: 0.8067 time: 0.0174s\n",
      "Epoch: 0002 loss_train: 0.2929 acc_train: 0.9643 loss_val: 0.6491 acc_val: 0.8067 time: 0.0149s\n",
      "Epoch: 0003 loss_train: 0.3438 acc_train: 0.9429 loss_val: 0.6487 acc_val: 0.8133 time: 0.0141s\n",
      "Epoch: 0004 loss_train: 0.3085 acc_train: 0.9500 loss_val: 0.6484 acc_val: 0.8067 time: 0.0153s\n",
      "Epoch: 0005 loss_train: 0.3192 acc_train: 0.9571 loss_val: 0.6477 acc_val: 0.8133 time: 0.0140s\n",
      "Epoch: 0006 loss_train: 0.2752 acc_train: 0.9500 loss_val: 0.6472 acc_val: 0.8100 time: 0.0137s\n",
      "Epoch: 0007 loss_train: 0.3063 acc_train: 0.9571 loss_val: 0.6458 acc_val: 0.8133 time: 0.0169s\n",
      "Epoch: 0008 loss_train: 0.3143 acc_train: 0.9643 loss_val: 0.6445 acc_val: 0.8133 time: 0.0155s\n",
      "Epoch: 0009 loss_train: 0.3233 acc_train: 0.9429 loss_val: 0.6432 acc_val: 0.8100 time: 0.0164s\n",
      "Epoch: 0010 loss_train: 0.2737 acc_train: 0.9786 loss_val: 0.6429 acc_val: 0.8100 time: 0.0161s\n",
      "Epoch: 0011 loss_train: 0.2659 acc_train: 0.9929 loss_val: 0.6434 acc_val: 0.8033 time: 0.0134s\n",
      "Epoch: 0012 loss_train: 0.3363 acc_train: 0.9286 loss_val: 0.6440 acc_val: 0.8033 time: 0.0133s\n",
      "Epoch: 0013 loss_train: 0.3236 acc_train: 0.9571 loss_val: 0.6454 acc_val: 0.8100 time: 0.0147s\n",
      "Epoch: 0014 loss_train: 0.2779 acc_train: 0.9714 loss_val: 0.6477 acc_val: 0.8100 time: 0.0142s\n",
      "Epoch: 0015 loss_train: 0.3238 acc_train: 0.9643 loss_val: 0.6487 acc_val: 0.8200 time: 0.0119s\n",
      "Epoch: 0016 loss_train: 0.3184 acc_train: 0.9571 loss_val: 0.6500 acc_val: 0.8133 time: 0.0130s\n",
      "Epoch: 0017 loss_train: 0.3015 acc_train: 0.9571 loss_val: 0.6497 acc_val: 0.8133 time: 0.0137s\n",
      "Epoch: 0018 loss_train: 0.2738 acc_train: 0.9929 loss_val: 0.6485 acc_val: 0.8133 time: 0.0141s\n",
      "Epoch: 0019 loss_train: 0.2818 acc_train: 0.9500 loss_val: 0.6479 acc_val: 0.7933 time: 0.0146s\n",
      "Epoch: 0020 loss_train: 0.2987 acc_train: 0.9643 loss_val: 0.6478 acc_val: 0.8000 time: 0.0211s\n",
      "Epoch: 0021 loss_train: 0.2950 acc_train: 0.9643 loss_val: 0.6476 acc_val: 0.8000 time: 0.0208s\n",
      "Epoch: 0022 loss_train: 0.2504 acc_train: 0.9786 loss_val: 0.6478 acc_val: 0.8000 time: 0.0160s\n",
      "Epoch: 0023 loss_train: 0.3134 acc_train: 0.9714 loss_val: 0.6464 acc_val: 0.7967 time: 0.0153s\n",
      "Epoch: 0024 loss_train: 0.3364 acc_train: 0.9500 loss_val: 0.6457 acc_val: 0.8033 time: 0.0123s\n",
      "Epoch: 0025 loss_train: 0.3073 acc_train: 0.9643 loss_val: 0.6454 acc_val: 0.8133 time: 0.0131s\n",
      "Epoch: 0026 loss_train: 0.3202 acc_train: 0.9286 loss_val: 0.6442 acc_val: 0.8100 time: 0.0133s\n",
      "Epoch: 0027 loss_train: 0.3200 acc_train: 0.9643 loss_val: 0.6424 acc_val: 0.8100 time: 0.0126s\n",
      "Epoch: 0028 loss_train: 0.3402 acc_train: 0.9571 loss_val: 0.6413 acc_val: 0.8133 time: 0.0117s\n",
      "Epoch: 0029 loss_train: 0.2793 acc_train: 0.9643 loss_val: 0.6412 acc_val: 0.8133 time: 0.0125s\n",
      "Epoch: 0030 loss_train: 0.3547 acc_train: 0.9286 loss_val: 0.6417 acc_val: 0.8133 time: 0.0130s\n",
      "Epoch: 0031 loss_train: 0.3053 acc_train: 0.9643 loss_val: 0.6429 acc_val: 0.8133 time: 0.0139s\n",
      "Epoch: 0032 loss_train: 0.3093 acc_train: 0.9571 loss_val: 0.6440 acc_val: 0.8133 time: 0.0134s\n",
      "Epoch: 0033 loss_train: 0.2810 acc_train: 0.9643 loss_val: 0.6456 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0034 loss_train: 0.3438 acc_train: 0.9429 loss_val: 0.6451 acc_val: 0.8133 time: 0.0126s\n",
      "Epoch: 0035 loss_train: 0.3222 acc_train: 0.9429 loss_val: 0.6425 acc_val: 0.8133 time: 0.0131s\n",
      "Epoch: 0036 loss_train: 0.3034 acc_train: 0.9857 loss_val: 0.6399 acc_val: 0.8133 time: 0.0103s\n",
      "Epoch: 0037 loss_train: 0.2700 acc_train: 0.9643 loss_val: 0.6380 acc_val: 0.8200 time: 0.0108s\n",
      "Epoch: 0038 loss_train: 0.3062 acc_train: 0.9357 loss_val: 0.6356 acc_val: 0.8133 time: 0.0109s\n",
      "Epoch: 0039 loss_train: 0.2617 acc_train: 0.9714 loss_val: 0.6345 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0040 loss_train: 0.2907 acc_train: 0.9571 loss_val: 0.6339 acc_val: 0.8133 time: 0.0108s\n",
      "Epoch: 0041 loss_train: 0.3027 acc_train: 0.9786 loss_val: 0.6345 acc_val: 0.8100 time: 0.0113s\n",
      "Epoch: 0042 loss_train: 0.2756 acc_train: 0.9786 loss_val: 0.6357 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0043 loss_train: 0.3004 acc_train: 0.9571 loss_val: 0.6376 acc_val: 0.8067 time: 0.0119s\n",
      "Epoch: 0044 loss_train: 0.3026 acc_train: 0.9643 loss_val: 0.6389 acc_val: 0.8100 time: 0.0114s\n",
      "Epoch: 0045 loss_train: 0.2986 acc_train: 0.9429 loss_val: 0.6402 acc_val: 0.8133 time: 0.0111s\n",
      "Epoch: 0046 loss_train: 0.2876 acc_train: 0.9571 loss_val: 0.6413 acc_val: 0.8100 time: 0.0115s\n",
      "Epoch: 0047 loss_train: 0.2945 acc_train: 0.9429 loss_val: 0.6411 acc_val: 0.8067 time: 0.0115s\n",
      "Epoch: 0048 loss_train: 0.2905 acc_train: 0.9643 loss_val: 0.6395 acc_val: 0.8067 time: 0.0122s\n",
      "Epoch: 0049 loss_train: 0.3007 acc_train: 0.9357 loss_val: 0.6374 acc_val: 0.8200 time: 0.0114s\n",
      "Epoch: 0050 loss_train: 0.3027 acc_train: 0.9643 loss_val: 0.6364 acc_val: 0.8167 time: 0.0114s\n",
      "Epoch: 0051 loss_train: 0.2883 acc_train: 0.9500 loss_val: 0.6360 acc_val: 0.8233 time: 0.0121s\n",
      "Epoch: 0052 loss_train: 0.3249 acc_train: 0.9571 loss_val: 0.6368 acc_val: 0.8233 time: 0.0130s\n",
      "Epoch: 0053 loss_train: 0.2875 acc_train: 0.9643 loss_val: 0.6373 acc_val: 0.8033 time: 0.0118s\n",
      "Epoch: 0054 loss_train: 0.2660 acc_train: 0.9786 loss_val: 0.6389 acc_val: 0.8067 time: 0.0129s\n",
      "Epoch: 0055 loss_train: 0.2588 acc_train: 0.9714 loss_val: 0.6402 acc_val: 0.8000 time: 0.0126s\n",
      "Epoch: 0056 loss_train: 0.2711 acc_train: 0.9643 loss_val: 0.6404 acc_val: 0.7967 time: 0.0126s\n",
      "Epoch: 0057 loss_train: 0.2705 acc_train: 0.9643 loss_val: 0.6398 acc_val: 0.8067 time: 0.0126s\n",
      "Epoch: 0058 loss_train: 0.2849 acc_train: 0.9643 loss_val: 0.6382 acc_val: 0.8133 time: 0.0130s\n",
      "Epoch: 0059 loss_train: 0.2876 acc_train: 0.9500 loss_val: 0.6364 acc_val: 0.8167 time: 0.0132s\n",
      "Epoch: 0060 loss_train: 0.2769 acc_train: 0.9643 loss_val: 0.6349 acc_val: 0.8167 time: 0.0126s\n",
      "Epoch: 0061 loss_train: 0.3014 acc_train: 0.9500 loss_val: 0.6339 acc_val: 0.8100 time: 0.0126s\n",
      "Epoch: 0062 loss_train: 0.3070 acc_train: 0.9571 loss_val: 0.6338 acc_val: 0.8133 time: 0.0129s\n",
      "Epoch: 0063 loss_train: 0.3007 acc_train: 0.9786 loss_val: 0.6335 acc_val: 0.8100 time: 0.0132s\n",
      "Epoch: 0064 loss_train: 0.2908 acc_train: 0.9643 loss_val: 0.6347 acc_val: 0.8133 time: 0.0131s\n",
      "Epoch: 0065 loss_train: 0.2577 acc_train: 0.9786 loss_val: 0.6358 acc_val: 0.8133 time: 0.0123s\n",
      "Epoch: 0066 loss_train: 0.2928 acc_train: 0.9429 loss_val: 0.6378 acc_val: 0.8100 time: 0.0130s\n",
      "Epoch: 0067 loss_train: 0.2990 acc_train: 0.9643 loss_val: 0.6395 acc_val: 0.8067 time: 0.0126s\n",
      "Epoch: 0068 loss_train: 0.3184 acc_train: 0.9500 loss_val: 0.6407 acc_val: 0.8067 time: 0.0119s\n",
      "Epoch: 0069 loss_train: 0.2810 acc_train: 0.9571 loss_val: 0.6392 acc_val: 0.8033 time: 0.0121s\n",
      "Epoch: 0070 loss_train: 0.2811 acc_train: 0.9571 loss_val: 0.6382 acc_val: 0.8067 time: 0.0128s\n",
      "Epoch: 0071 loss_train: 0.2629 acc_train: 0.9786 loss_val: 0.6370 acc_val: 0.8200 time: 0.0130s\n",
      "Epoch: 0072 loss_train: 0.2894 acc_train: 0.9500 loss_val: 0.6358 acc_val: 0.8133 time: 0.0119s\n",
      "Epoch: 0073 loss_train: 0.2537 acc_train: 0.9786 loss_val: 0.6336 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0074 loss_train: 0.3123 acc_train: 0.9286 loss_val: 0.6306 acc_val: 0.8167 time: 0.0125s\n",
      "Epoch: 0075 loss_train: 0.2914 acc_train: 0.9571 loss_val: 0.6278 acc_val: 0.8167 time: 0.0122s\n",
      "Epoch: 0076 loss_train: 0.2651 acc_train: 0.9857 loss_val: 0.6266 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0077 loss_train: 0.2772 acc_train: 0.9643 loss_val: 0.6275 acc_val: 0.8100 time: 0.0121s\n",
      "Epoch: 0078 loss_train: 0.2962 acc_train: 0.9429 loss_val: 0.6295 acc_val: 0.8100 time: 0.0126s\n",
      "Epoch: 0079 loss_train: 0.2984 acc_train: 0.9714 loss_val: 0.6310 acc_val: 0.8067 time: 0.0131s\n",
      "Epoch: 0080 loss_train: 0.3030 acc_train: 0.9571 loss_val: 0.6332 acc_val: 0.8100 time: 0.0124s\n",
      "Epoch: 0081 loss_train: 0.2501 acc_train: 0.9714 loss_val: 0.6349 acc_val: 0.8067 time: 0.0122s\n",
      "Epoch: 0082 loss_train: 0.3211 acc_train: 0.9643 loss_val: 0.6345 acc_val: 0.8067 time: 0.0129s\n",
      "Epoch: 0083 loss_train: 0.3038 acc_train: 0.9571 loss_val: 0.6341 acc_val: 0.8033 time: 0.0127s\n",
      "Epoch: 0084 loss_train: 0.2793 acc_train: 0.9500 loss_val: 0.6327 acc_val: 0.8100 time: 0.0114s\n",
      "Epoch: 0085 loss_train: 0.2597 acc_train: 0.9714 loss_val: 0.6310 acc_val: 0.8100 time: 0.0115s\n",
      "Epoch: 0086 loss_train: 0.2590 acc_train: 0.9929 loss_val: 0.6296 acc_val: 0.8067 time: 0.0114s\n",
      "Epoch: 0087 loss_train: 0.2778 acc_train: 0.9571 loss_val: 0.6292 acc_val: 0.8100 time: 0.0109s\n",
      "Epoch: 0088 loss_train: 0.3014 acc_train: 0.9429 loss_val: 0.6286 acc_val: 0.8067 time: 0.0111s\n",
      "Epoch: 0089 loss_train: 0.2725 acc_train: 0.9643 loss_val: 0.6297 acc_val: 0.8033 time: 0.0106s\n",
      "Epoch: 0090 loss_train: 0.2672 acc_train: 0.9643 loss_val: 0.6329 acc_val: 0.8033 time: 0.0106s\n",
      "Epoch: 0091 loss_train: 0.2672 acc_train: 0.9571 loss_val: 0.6364 acc_val: 0.8000 time: 0.0107s\n",
      "Epoch: 0092 loss_train: 0.2883 acc_train: 0.9643 loss_val: 0.6372 acc_val: 0.7967 time: 0.0112s\n",
      "Epoch: 0093 loss_train: 0.2925 acc_train: 0.9500 loss_val: 0.6366 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0094 loss_train: 0.2691 acc_train: 0.9714 loss_val: 0.6347 acc_val: 0.8033 time: 0.0103s\n",
      "Epoch: 0095 loss_train: 0.2732 acc_train: 0.9714 loss_val: 0.6345 acc_val: 0.8033 time: 0.0107s\n",
      "Epoch: 0096 loss_train: 0.2902 acc_train: 0.9571 loss_val: 0.6346 acc_val: 0.8133 time: 0.0114s\n",
      "Epoch: 0097 loss_train: 0.3027 acc_train: 0.9500 loss_val: 0.6356 acc_val: 0.8133 time: 0.0109s\n",
      "Epoch: 0098 loss_train: 0.2735 acc_train: 0.9643 loss_val: 0.6350 acc_val: 0.8100 time: 0.0117s\n",
      "Epoch: 0099 loss_train: 0.2975 acc_train: 0.9429 loss_val: 0.6342 acc_val: 0.8067 time: 0.0112s\n",
      "Epoch: 0100 loss_train: 0.2827 acc_train: 0.9714 loss_val: 0.6339 acc_val: 0.8000 time: 0.0109s\n",
      "Epoch: 0101 loss_train: 0.2744 acc_train: 0.9643 loss_val: 0.6337 acc_val: 0.8000 time: 0.0111s\n",
      "Epoch: 0102 loss_train: 0.2573 acc_train: 0.9643 loss_val: 0.6353 acc_val: 0.8033 time: 0.0102s\n",
      "Epoch: 0103 loss_train: 0.2764 acc_train: 0.9643 loss_val: 0.6376 acc_val: 0.8033 time: 0.0106s\n",
      "Epoch: 0104 loss_train: 0.2692 acc_train: 0.9571 loss_val: 0.6383 acc_val: 0.8033 time: 0.0113s\n",
      "Epoch: 0105 loss_train: 0.2679 acc_train: 0.9643 loss_val: 0.6366 acc_val: 0.8033 time: 0.0112s\n",
      "Epoch: 0106 loss_train: 0.2593 acc_train: 0.9714 loss_val: 0.6323 acc_val: 0.8033 time: 0.0103s\n",
      "Epoch: 0107 loss_train: 0.2591 acc_train: 0.9714 loss_val: 0.6284 acc_val: 0.8067 time: 0.0105s\n",
      "Epoch: 0108 loss_train: 0.3310 acc_train: 0.9571 loss_val: 0.6258 acc_val: 0.8100 time: 0.0114s\n",
      "Epoch: 0109 loss_train: 0.2647 acc_train: 0.9500 loss_val: 0.6248 acc_val: 0.8067 time: 0.0116s\n",
      "Epoch: 0110 loss_train: 0.2686 acc_train: 0.9857 loss_val: 0.6259 acc_val: 0.8067 time: 0.0117s\n",
      "Epoch: 0111 loss_train: 0.2852 acc_train: 0.9500 loss_val: 0.6265 acc_val: 0.8067 time: 0.0105s\n",
      "Epoch: 0112 loss_train: 0.2550 acc_train: 0.9786 loss_val: 0.6261 acc_val: 0.8067 time: 0.0112s\n",
      "Epoch: 0113 loss_train: 0.2779 acc_train: 0.9643 loss_val: 0.6270 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0114 loss_train: 0.2651 acc_train: 0.9857 loss_val: 0.6286 acc_val: 0.8033 time: 0.0134s\n",
      "Epoch: 0115 loss_train: 0.2802 acc_train: 0.9643 loss_val: 0.6303 acc_val: 0.8100 time: 0.0115s\n",
      "Epoch: 0116 loss_train: 0.2582 acc_train: 0.9714 loss_val: 0.6317 acc_val: 0.8033 time: 0.0116s\n",
      "Epoch: 0117 loss_train: 0.2786 acc_train: 0.9714 loss_val: 0.6327 acc_val: 0.8067 time: 0.0131s\n",
      "Epoch: 0118 loss_train: 0.2670 acc_train: 0.9643 loss_val: 0.6333 acc_val: 0.8067 time: 0.0122s\n",
      "Epoch: 0119 loss_train: 0.2827 acc_train: 0.9857 loss_val: 0.6321 acc_val: 0.8033 time: 0.0104s\n",
      "Epoch: 0120 loss_train: 0.2851 acc_train: 0.9429 loss_val: 0.6286 acc_val: 0.8133 time: 0.0109s\n",
      "Epoch: 0121 loss_train: 0.2616 acc_train: 0.9857 loss_val: 0.6265 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0122 loss_train: 0.2772 acc_train: 0.9286 loss_val: 0.6252 acc_val: 0.8167 time: 0.0127s\n",
      "Epoch: 0123 loss_train: 0.3037 acc_train: 0.9714 loss_val: 0.6243 acc_val: 0.8133 time: 0.0113s\n",
      "Epoch: 0124 loss_train: 0.2711 acc_train: 0.9571 loss_val: 0.6250 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0125 loss_train: 0.2721 acc_train: 0.9643 loss_val: 0.6258 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0126 loss_train: 0.2598 acc_train: 0.9643 loss_val: 0.6275 acc_val: 0.8067 time: 0.0114s\n",
      "Epoch: 0127 loss_train: 0.2468 acc_train: 0.9929 loss_val: 0.6280 acc_val: 0.8067 time: 0.0112s\n",
      "Epoch: 0128 loss_train: 0.2508 acc_train: 0.9929 loss_val: 0.6281 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0129 loss_train: 0.2755 acc_train: 0.9857 loss_val: 0.6267 acc_val: 0.8067 time: 0.0114s\n",
      "Epoch: 0130 loss_train: 0.2688 acc_train: 0.9500 loss_val: 0.6252 acc_val: 0.8100 time: 0.0118s\n",
      "Epoch: 0131 loss_train: 0.3059 acc_train: 0.9429 loss_val: 0.6249 acc_val: 0.8100 time: 0.0123s\n",
      "Epoch: 0132 loss_train: 0.2636 acc_train: 0.9714 loss_val: 0.6247 acc_val: 0.8133 time: 0.0111s\n",
      "Epoch: 0133 loss_train: 0.2811 acc_train: 0.9643 loss_val: 0.6250 acc_val: 0.8133 time: 0.0111s\n",
      "Epoch: 0134 loss_train: 0.2537 acc_train: 0.9714 loss_val: 0.6271 acc_val: 0.8100 time: 0.0121s\n",
      "Epoch: 0135 loss_train: 0.2533 acc_train: 0.9857 loss_val: 0.6297 acc_val: 0.8100 time: 0.0114s\n",
      "Epoch: 0136 loss_train: 0.2638 acc_train: 0.9571 loss_val: 0.6325 acc_val: 0.8067 time: 0.0109s\n",
      "Epoch: 0137 loss_train: 0.2653 acc_train: 0.9714 loss_val: 0.6336 acc_val: 0.8033 time: 0.0115s\n",
      "Epoch: 0138 loss_train: 0.2864 acc_train: 0.9786 loss_val: 0.6322 acc_val: 0.8100 time: 0.0114s\n",
      "Epoch: 0139 loss_train: 0.2473 acc_train: 0.9714 loss_val: 0.6302 acc_val: 0.8167 time: 0.0116s\n",
      "Epoch: 0140 loss_train: 0.2571 acc_train: 0.9571 loss_val: 0.6278 acc_val: 0.8167 time: 0.0115s\n",
      "Epoch: 0141 loss_train: 0.2918 acc_train: 0.9571 loss_val: 0.6265 acc_val: 0.8167 time: 0.0116s\n",
      "Epoch: 0142 loss_train: 0.2566 acc_train: 0.9786 loss_val: 0.6256 acc_val: 0.8100 time: 0.0119s\n",
      "Epoch: 0143 loss_train: 0.2911 acc_train: 0.9643 loss_val: 0.6249 acc_val: 0.8100 time: 0.0117s\n",
      "Epoch: 0144 loss_train: 0.2825 acc_train: 0.9643 loss_val: 0.6249 acc_val: 0.8167 time: 0.0117s\n",
      "Epoch: 0145 loss_train: 0.2298 acc_train: 0.9786 loss_val: 0.6254 acc_val: 0.8133 time: 0.0114s\n",
      "Epoch: 0146 loss_train: 0.2870 acc_train: 0.9571 loss_val: 0.6283 acc_val: 0.8133 time: 0.0123s\n",
      "Epoch: 0147 loss_train: 0.2619 acc_train: 0.9571 loss_val: 0.6313 acc_val: 0.8100 time: 0.0121s\n",
      "Epoch: 0148 loss_train: 0.2683 acc_train: 0.9714 loss_val: 0.6326 acc_val: 0.8167 time: 0.0121s\n",
      "Epoch: 0149 loss_train: 0.2938 acc_train: 0.9786 loss_val: 0.6318 acc_val: 0.8133 time: 0.0115s\n",
      "Epoch: 0150 loss_train: 0.2291 acc_train: 0.9714 loss_val: 0.6296 acc_val: 0.8167 time: 0.0117s\n",
      "Epoch: 0151 loss_train: 0.2563 acc_train: 0.9357 loss_val: 0.6274 acc_val: 0.8133 time: 0.0124s\n",
      "Epoch: 0152 loss_train: 0.2936 acc_train: 0.9571 loss_val: 0.6266 acc_val: 0.8133 time: 0.0119s\n",
      "Epoch: 0153 loss_train: 0.2588 acc_train: 0.9714 loss_val: 0.6256 acc_val: 0.8033 time: 0.0112s\n",
      "Epoch: 0154 loss_train: 0.2718 acc_train: 0.9786 loss_val: 0.6258 acc_val: 0.8000 time: 0.0115s\n",
      "Epoch: 0155 loss_train: 0.2759 acc_train: 0.9714 loss_val: 0.6265 acc_val: 0.8033 time: 0.0118s\n",
      "Epoch: 0156 loss_train: 0.2636 acc_train: 0.9643 loss_val: 0.6275 acc_val: 0.8133 time: 0.0116s\n",
      "Epoch: 0157 loss_train: 0.2895 acc_train: 0.9786 loss_val: 0.6293 acc_val: 0.8167 time: 0.0112s\n",
      "Epoch: 0158 loss_train: 0.2703 acc_train: 0.9714 loss_val: 0.6297 acc_val: 0.8167 time: 0.0115s\n",
      "Epoch: 0159 loss_train: 0.2760 acc_train: 0.9571 loss_val: 0.6300 acc_val: 0.8133 time: 0.0116s\n",
      "Epoch: 0160 loss_train: 0.2759 acc_train: 0.9714 loss_val: 0.6309 acc_val: 0.8133 time: 0.0118s\n",
      "Epoch: 0161 loss_train: 0.2515 acc_train: 0.9857 loss_val: 0.6314 acc_val: 0.8133 time: 0.0112s\n",
      "Epoch: 0162 loss_train: 0.2526 acc_train: 0.9571 loss_val: 0.6305 acc_val: 0.8133 time: 0.0116s\n",
      "Epoch: 0163 loss_train: 0.2318 acc_train: 0.9714 loss_val: 0.6293 acc_val: 0.8167 time: 0.0115s\n",
      "Epoch: 0164 loss_train: 0.2653 acc_train: 0.9643 loss_val: 0.6286 acc_val: 0.8133 time: 0.0116s\n",
      "Epoch: 0165 loss_train: 0.2753 acc_train: 0.9643 loss_val: 0.6290 acc_val: 0.8100 time: 0.0119s\n",
      "Epoch: 0166 loss_train: 0.2797 acc_train: 0.9500 loss_val: 0.6298 acc_val: 0.8100 time: 0.0113s\n",
      "Epoch: 0167 loss_train: 0.2662 acc_train: 0.9714 loss_val: 0.6304 acc_val: 0.8100 time: 0.0116s\n",
      "Epoch: 0168 loss_train: 0.2421 acc_train: 0.9929 loss_val: 0.6297 acc_val: 0.8100 time: 0.0124s\n",
      "Epoch: 0169 loss_train: 0.2788 acc_train: 0.9643 loss_val: 0.6283 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0170 loss_train: 0.2519 acc_train: 0.9857 loss_val: 0.6271 acc_val: 0.8033 time: 0.0113s\n",
      "Epoch: 0171 loss_train: 0.2626 acc_train: 0.9571 loss_val: 0.6267 acc_val: 0.8033 time: 0.0116s\n",
      "Epoch: 0172 loss_train: 0.2471 acc_train: 0.9571 loss_val: 0.6272 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0173 loss_train: 0.2609 acc_train: 0.9786 loss_val: 0.6272 acc_val: 0.8067 time: 0.0115s\n",
      "Epoch: 0174 loss_train: 0.2462 acc_train: 0.9571 loss_val: 0.6277 acc_val: 0.8067 time: 0.0109s\n",
      "Epoch: 0175 loss_train: 0.2960 acc_train: 0.9286 loss_val: 0.6285 acc_val: 0.8067 time: 0.0113s\n",
      "Epoch: 0176 loss_train: 0.2523 acc_train: 0.9714 loss_val: 0.6284 acc_val: 0.8067 time: 0.0117s\n",
      "Epoch: 0177 loss_train: 0.2385 acc_train: 0.9857 loss_val: 0.6287 acc_val: 0.8100 time: 0.0117s\n",
      "Epoch: 0178 loss_train: 0.2435 acc_train: 0.9643 loss_val: 0.6295 acc_val: 0.8100 time: 0.0108s\n",
      "Epoch: 0179 loss_train: 0.2241 acc_train: 0.9714 loss_val: 0.6292 acc_val: 0.8100 time: 0.0114s\n",
      "Epoch: 0180 loss_train: 0.2584 acc_train: 0.9429 loss_val: 0.6282 acc_val: 0.8100 time: 0.0111s\n",
      "Epoch: 0181 loss_train: 0.2733 acc_train: 0.9643 loss_val: 0.6264 acc_val: 0.8100 time: 0.0114s\n",
      "Epoch: 0182 loss_train: 0.2434 acc_train: 0.9571 loss_val: 0.6250 acc_val: 0.8100 time: 0.0111s\n",
      "Epoch: 0183 loss_train: 0.2436 acc_train: 0.9643 loss_val: 0.6245 acc_val: 0.8067 time: 0.0114s\n",
      "Epoch: 0184 loss_train: 0.2387 acc_train: 0.9857 loss_val: 0.6231 acc_val: 0.8067 time: 0.0112s\n",
      "Epoch: 0185 loss_train: 0.2309 acc_train: 0.9786 loss_val: 0.6219 acc_val: 0.8100 time: 0.0117s\n",
      "Epoch: 0186 loss_train: 0.2720 acc_train: 0.9500 loss_val: 0.6221 acc_val: 0.8067 time: 0.0116s\n",
      "Epoch: 0187 loss_train: 0.2544 acc_train: 0.9571 loss_val: 0.6222 acc_val: 0.8067 time: 0.0106s\n",
      "Epoch: 0188 loss_train: 0.2364 acc_train: 0.9714 loss_val: 0.6230 acc_val: 0.8033 time: 0.0117s\n",
      "Epoch: 0189 loss_train: 0.2599 acc_train: 0.9714 loss_val: 0.6247 acc_val: 0.8033 time: 0.0118s\n",
      "Epoch: 0190 loss_train: 0.2421 acc_train: 0.9714 loss_val: 0.6265 acc_val: 0.8033 time: 0.0116s\n",
      "Epoch: 0191 loss_train: 0.2569 acc_train: 0.9571 loss_val: 0.6289 acc_val: 0.8067 time: 0.0108s\n",
      "Epoch: 0192 loss_train: 0.2349 acc_train: 0.9571 loss_val: 0.6310 acc_val: 0.8033 time: 0.0109s\n",
      "Epoch: 0193 loss_train: 0.2482 acc_train: 0.9643 loss_val: 0.6315 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0194 loss_train: 0.2328 acc_train: 0.9857 loss_val: 0.6308 acc_val: 0.8033 time: 0.0112s\n",
      "Epoch: 0195 loss_train: 0.2638 acc_train: 0.9571 loss_val: 0.6286 acc_val: 0.8100 time: 0.0113s\n",
      "Epoch: 0196 loss_train: 0.2517 acc_train: 0.9571 loss_val: 0.6255 acc_val: 0.8067 time: 0.0106s\n",
      "Epoch: 0197 loss_train: 0.2613 acc_train: 0.9714 loss_val: 0.6225 acc_val: 0.8067 time: 0.0114s\n",
      "Epoch: 0198 loss_train: 0.2281 acc_train: 0.9857 loss_val: 0.6212 acc_val: 0.8100 time: 0.0112s\n",
      "Epoch: 0199 loss_train: 0.2536 acc_train: 0.9714 loss_val: 0.6203 acc_val: 0.8067 time: 0.0116s\n",
      "Epoch: 0200 loss_train: 0.2553 acc_train: 0.9643 loss_val: 0.6206 acc_val: 0.8067 time: 0.0116s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.4752s\n",
      "Test set results: loss= 0.6153 accuracy= 0.8370\n"
     ]
    }
   ],
   "source": [
    "def train(epoch = 200):\n",
    "    acc_train_arr, loss_train_arr, acc_valid_arr, loss_valid_arr = [], [], [], []\n",
    "\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    fastmode = False #Validate during training pass\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(200):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}